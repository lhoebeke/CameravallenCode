{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to classify new sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a tutorial that shows you how to classify new sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data\n",
    "* Camera trap images, in folders per deployment\n",
    "* Agouti export files: observations, assets and pickup-setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the predefined configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the paths to the different folders. <br>\n",
    "If you have the same data structure as in this repository, you can use the predefined configuration file. If needed, you can change the paths according to you own folder structure.\n",
    "\n",
    "The following paths are defined:\n",
    "* **general_folder_path** : orginal camera trap images and the Agouti export files (assets, observations and pickup-setup)\n",
    "* **resized_folder_path** : resized camera trap images\n",
    "* **preprocessing_output_path** : preprocessing output\n",
    "* **crop_output_path** : cropped images (optional)\n",
    "* **draw_output_path** : regions of interest indicated on original camera trap images (optional)\n",
    "* **bottleneck_features_output_path** : extracted bottleneck features\n",
    "* **weight_path** : weights top model\n",
    "* **predictions_output_path** : predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load\n",
    "\n",
    "with open(\"config.yml\") as yaml_config:\n",
    "    config = load(yaml_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all folders exist and create folder if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for path in config:\n",
    "    if not os.path.exists(config[path]):\n",
    "        os.makedirs(config[path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: resize images\n",
    "The camera trap images are resized to 50% of their original size. This strongly decreases the computational time, while the performance remains the same.<br>\n",
    "\n",
    "Input: original camera trap images and Agouti export file (observations)<br>\n",
    "Output: resized camera trap images in a similar folder structure as the original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing.Resize_images import resize_images\n",
    "\n",
    "resize_images(config[\"general_folder_path\"], \n",
    "              config[\"resized_folder_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: preprocess images\n",
    "During the preprocessing, the regions of interest in the images are determined. All images of a sequence are used to construct a background image. Subsequently, the regions of interest in a camera trap image are determined by computing the difference between this background image and the camera trap image.\n",
    "\n",
    "Input: resized camera trap images and Agouti export files(observations + assets + pickupsetup)<br>\n",
    "Output: cvs-file containing the coordinates of the regions of interest in every camera trap image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing.Preprocessing import preprocessing\n",
    "\n",
    "preprocessing(config[\"general_folder_path\"], \n",
    "              config[\"resized_folder_path\"], \n",
    "              config[\"preprocessing_output_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: crop images or indicate regions of interest\n",
    "We can crop the camera trap images or indicate the regions of interest on the camera trap images to see the result of the preprocessing. <br>\n",
    "This step is optional and not required to classify the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing.Crop_images import crop_images\n",
    "from Preprocessing.Draw_boxes import draw_boxes\n",
    "\n",
    "crop_images(config[\"preprocessing_output_path\"], config[\"resized_folder_path\"], config[\"crop_output_path\"])\n",
    "draw_boxes(config[\"preprocessing_output_path\"], config[\"resized_folder_path\"], config[\"draw_output_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import image\n",
    "\n",
    "# Show a camera trap image with indication of the regions of interest\n",
    "deployment = os.listdir(config[\"draw_output_path\"])[0]\n",
    "image_name = os.listdir(deployment)[0]\n",
    "image = Image.open(os.path.join(config[\"draw_output_path\"],deployment, image_name))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: extract bottleneck features\n",
    "\n",
    "The pretrained convolutional neural network ResNet50 is used to convert the images to bottleneck features.\n",
    "\n",
    "Input: resized camera trap images and preprocessing output containing the coordinates of the boxes <br>\n",
    "Output: bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Network.ResNet50_Bottleneck_features_Predict import extract_bottleneck_features\n",
    "\n",
    "extract_bottleneck_features(config[\"preprocessing_output_path\"], \n",
    "                            config[\"bottleneck_features_output_path\"], \n",
    "                            config[\"resized_folder_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : run top model to classify the new images\n",
    "\n",
    "The extracted bottleneck features are fed to the new top model to predict the labels of the new images.\n",
    "\n",
    "Input: extracted bottleneck features <br>\n",
    "Ouput: probabilities of the output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Network.ResNet50_Hierarchical_Bottleneck_Predict import hierarchical_bottleneck_predict\n",
    "\n",
    "hierarchical_bottleneck_predict(config[\"bottleneck_features_output_path\"], \n",
    "                                config[\"weight_path\"], \n",
    "                                config[\"predictions_output_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : convert output probabilities to hierarchical classification\n",
    "\n",
    "The predictions of the individual images are aggregated to a hierarchical prediction for every sequence.\n",
    "\n",
    "Input: probabilities of the output classes for the individual images <br>\n",
    "Output: hierarchical classification of the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Network.Hierachical_processing_predictions import hierarchical_predictions_sequences\n",
    "\n",
    "hierarchical_predictions_sequences( config[\"predictions_output_path\"], \n",
    "                                   config[\"bottleneck_features_output_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the file containing the hierarchical predictions to see the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predictions = pd.read_csv(os.path.join(config[\"predictions_output_path\"],'hierarchical_predictions_sequences.csv'), sep = ';')\n",
    "predictions.drop(['level_1_p','level_2_p','level_3_p','level_4_p','level_5_p'], axis=1, inplace=True)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Object localization\n",
    "\n",
    "The class activation maps can be used to localize the objects in the cropped camera trap images. <br>\n",
    "Since this step uses the cropped images, make sure to first run the optional cropping step above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Network.ResNet_CAM import object_localization\n",
    "\n",
    "img_path = os.path.join(config[\"crop_output_path\"], os.listdir(config[\"crop_output_path\"])[0])\n",
    "object_localization(img_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
